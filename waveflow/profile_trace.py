#!/usr/bin/env python3

# profile_trace.py 
# 
# Copyright (c) 2010-2018 Wave Computing, Inc. and its applicable licensors.   
# All rights reserved; provided, that any files identified as open source shall
# be governed by the specific open source license(s) applicable to such files. 
#
# For any files associated with distributions under the Apache 2.0 license, 
# full attribution to The Apache Software Foundation is given via the license 
# below.
#
# PURPOSE
#      Analyze traces generated by Tensorflow of model execution, and provide statistics 
#   about operators and time.
# 
# Author          : Ken Shiring
# Created On      : 04/18/2018
# 
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 # 
 #     http://www.apache.org/licenses/LICENSE-2.0
 # 
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.

import os
import sys
import json
# import configargparse as cap
import argparse
import functools

import numpy as np
import pandas as pd


class OpTiming(object):
    ''' This class is used to capture timing data for a specific entity (e.g. op, pass)
        over 1 or more events.
    '''
    def __init__(self, ename, optype, tags):
        self._ename = ename
        self._type = optype
        self._tags = tags
        self._start = []
        self._dur = []

    def __repr__(self):
        return "Name: %s, Type: %s, Tags: %s, Starts: %s, Durations: %s" % (self._ename, self._type, 
            str(self._tags), str(self._start), str(self._dur))

    def get_name(self):
        return self._ename

    def get_type(self):
        return self._type

    def is_pass(self):
        return self._type == 'Pass'

    def add_event(self, e, tend=None):
        assert 'args' in e
        s = int(e['ts'])
        self._start.append(s)
        # If the user specifies an end time, then we can't simply use the duration field.
        if tend:
            d = int(tend) - s
        else:
            d = e['dur']
        assert d > 0
        self._dur.append(int(d))

    def _median(self):
        # Compute the median value among all op invocations.
        return np.median(self._dur)

    def get_timing(self):
        return self._median()



class NetworkTrace(object):

    def __init__(self, jfile, matching_ops):
        self._jfile = jfile
        self._all_events = None
        self._passes = 0
        self._matching_ops = matching_ops
        self._debug = True

        self._matching_ops.append('_SOURCE')
        self._matching_ops.append('_Retval')


    def op_name(self, e):
        if not 'args' in e:
            return None
        if self.is_fwd(e):
            return 'Fwd'
        elif self.is_bwd(e):
            return 'Bwd'
        return e['args']['name']

    def load_trace(self):
        ''' Load the JSON formatted trace file generated by TF.
        '''
        if not os.path.isfile(self._jfile):
            print("Error: can't find file %s" % (self._jfile))
            return False

        with open(self._jfile, 'r') as f:
            jsdata = json.load(f)
            f.close()

        self._all_events = jsdata['traceEvents']

        def count_passes(events):
            count = 0
            for e in events:
                if self.is_fwd(e):
                    count = count + 1
            return count

        # See if this is a individual or unified trace.
        self._passes = count_passes(self._all_events)
        print("Trace contains %d events, %d model passes" % (len(self._all_events), self._passes))
        return True

    def pass_iterator(self):
        for i, e in enumerate(self._all_events):
            if self.is_fwd(e):
                yield i

    def pass_end(self, start):
        for i in range(start+1, len(self._all_events)):
            e = self._all_events[i]
            if self.get_optype(e) == 'NoOp':
                return i
        return len(self._all_events)

    def get_optype(self, e):
        if not 'args' in e:
            return None
        # Handle special cases. Certain OpTypes are labelled as uninteresting, but still important.
        if self.is_fwd(e):
            return 'Pass'
        elif self.is_bwd(e):
            return 'Pass'
        return e['args']['op']

    def is_fwd(self, e):
        # We can't call op_name() here due to circular dependency.
        if not 'args' in e:
            return None
        return e['args']['name'] == '_SOURCE'

    def is_bwd(self, e):
        # We can't call get_optype() here due to circular dependency.
        if not 'args' in e:
            return None
        return e['args']['op'] == '_Retval'

    def op_matches(self, e):
        return self.get_optype(e) in self._matching_ops

    def process_trace(self):
        # Process 1 pass at a time. There may be other events annotated, but we only need to
        # count the ones inside of a fwd/bwd pass of a DNN.
        def add_op_event(e, opset, pdir, tend=None, dbg=None):
            op_name = self.op_name(e)
            if not op_name in opset:
                if dbg: print("Adding %s to map" % (op_name))
                opset[op_name] = OpTiming(op_name, optype=self.get_optype(e), tags={'dir':pdir})
            s_op = opset[op_name]
            s_op.add_event(e, tend)
            if self._debug: print("Adding %s event" % (op_name))
            return s_op

        significant_ops = {}
        for p in self.pass_iterator():
            pdir = 'Fwd'
            end = self.pass_end(p)
            if self._debug: print("Pass start: (%d, %d), fwd" % (p, end))
            tend = self._all_events[end]['ts']
            add_op_event(self._all_events[p], significant_ops, pdir='Fwd', tend=tend, dbg=self._debug)
            for i in range(p, self.pass_end(p)):
                e = self._all_events[i]
                if self._debug: print("Event: %s" % (e))
                if self.is_bwd(e):
                    pdir = 'Bwd'
                    if self._debug: print("Pass %d, bwd" % (p))
                    add_op_event(e, significant_ops, pdir='Bwd', tend=tend, dbg=self._debug)
                if not self.op_matches(e):
                    continue
                op_name = self.op_name(e)
                if self._debug: print("Op %s matches" % (op_name))
                s_op = add_op_event(e, significant_ops, pdir=pdir, dbg=self._debug)
        return significant_ops

    def build_table(self, significant_ops):
        # Transform the set of significant ops into a table.
        table_cols = ['Op', 'Op Type', 'Pass', 'CPU Time (us)', 'Cumulative Time (us/pass)', 'Cumulative Time (%/pass)']
        table_rows = len(significant_ops)
        # tdata = np.zeros(shape=(table_rows, len(table_cols)), dtype=np.float32)
        # table = pd.DataFrame(tdata, columns=table_cols)

        # First build an ordered list of ops. We need to order them by time, with special cases for
        # special events.
        def op_compare(a, b):
            if a._tags['dir'] != b._tags['dir']:
                if a._tags['dir'] == 'Fwd':
                    return -1
                return 1
            # For ops with the same dir, full pass metrics come first.
            if a._ename == 'Fwd':
                return -1
            elif b._ename == 'Fwd':
                return 1
            if a._ename == 'Bwd':
                return -1
            elif b._ename == 'Bwd':
                return 1
            # Equal dir, compare by timestamp.
            if a._start < b._start:
                return -1
            elif a._start > b._start:
                return 1
            return 0

        all_ops = [significant_ops[x] for x in significant_ops.keys()]
        # all_ops.sort(cmp=op_compare)
        sorted_ops = sorted(all_ops, key=functools.cmp_to_key(op_compare))
        if self._debug: 
            print("Sorted ops:")
            for o in sorted_ops:
                print("%s" % (str(o)))

        table = []
        col_op_time = table_cols[3]
        col_cum_time = table_cols[4]
        col_cum_pct = table_cols[5]
        cum_time = 0
        pass_time = 0
        for i, op in enumerate(sorted_ops):
            oname = op.get_name()
            cols = {}
            cols['Op Type'] = op.get_type()
            cols['Op'] = op.get_name()
            cols['Pass'] = op._tags['dir']
            cols[col_op_time] = op.get_timing()
            if op.is_pass():
                cum_time = 0
                pass_time = op.get_timing()
            else:
                cum_time += op.get_timing()
            cols[col_cum_time] = cum_time
            cols[col_cum_pct] = float(cum_time)/pass_time*100.0
            if self._debug: print("Adding %s to table:\n%s" % (oname, str(cols)))
            table.append(cols)

        df = pd.DataFrame(table, columns=table_cols)
        if self._debug: print("Dataframe:\n%s" % (df))
        return df

    def write_table(self, table, outfile):
        print("Writing summary to %s" % (outfile))
        table.to_csv(outfile)

    def generate_data(self, outfile):
        ops = self.process_trace()
        if self._debug: 
            print("total ops: %d" % (len(ops)))
            print("ops: %s" % (ops))
        t = self.build_table(ops)
        self.write_table(t, outfile)





parser = argparse.ArgumentParser(description='Extract performance data from TF traces')
parser.add_argument("-t",  "--trace", type=str, default='timeline.json', help='TF trace file to analyze')
parser.add_argument("-o",  "--output", type=str, default='report.csv', help='Report table to generate')

args = parser.parse_args()

t = NetworkTrace(args.trace, matching_ops=['MatMul', 'WaveMatMul', 'Conv2D', 'WaveConv2D',
    'WaveConv2DGradientInput', 'WaveConv2DGradientWeight'])
if not t.load_trace():
    print("Failed to load the performance trace, quitting.")
    sys.exit(1)

t.generate_data(args.output)

